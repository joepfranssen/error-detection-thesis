{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19152,"status":"ok","timestamp":1654529257620,"user":{"displayName":"Joep Franssen","userId":"14265939170258113406"},"user_tz":-120},"id":"ZNiGhef9E3vP","outputId":"5c9965a5-5843-4db0-87e4-63b929dad564"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":943,"status":"ok","timestamp":1654529309216,"user":{"displayName":"Joep Franssen","userId":"14265939170258113406"},"user_tz":-120},"id":"vmgJfEtvE5SL","outputId":"c4bc1f8f-1a60-4914-fa5d-750732e4b29e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Applied Data Science/Thesis/Code\n"]}],"source":["cd '/content/drive/MyDrive/Applied Data Science/Thesis/Code'"]},{"cell_type":"markdown","source":["Import libraries and install transformers"],"metadata":{"id":"1Zl_qVvYUO64"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"0oXqNb9PWRR1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654529322691,"user_tz":-120,"elapsed":9413,"user":{"displayName":"Joep Franssen","userId":"14265939170258113406"}},"outputId":"d507e87f-f59b-4cdf-aad5-977691723178"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 24.6 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 57.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWgYf_yGFCQZ"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFDistilBertForSequenceClassification\n","import numpy as np\n","from sklearn.metrics import classification_report \n","from sklearn.metrics import confusion_matrix\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","source":["Load csv files of retracted and non-retracted articles"],"metadata":{"id":"NS3LBjiHUU9-"}},{"cell_type":"code","source":["four_journal_train_data_set = pd.read_csv('/content/drive/MyDrive/Applied Data Science/Thesis/Code/Data (CSV)/four_journal_train_data_set.csv', encoding=\"utf-8-sig\")\n","two_journal_test_data_set = pd.read_csv('/content/drive/MyDrive/Applied Data Science/Thesis/Code/Data (CSV)/two_journal_test_data_set.csv', encoding=\"utf-8-sig\")"],"metadata":{"id":"6oF8r8W2UaJr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspect the columns"],"metadata":{"id":"nOXiF0HEWdG1"}},{"cell_type":"code","source":["four_journal_train_data_set.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4O71Yim6WsOj","executionInfo":{"status":"ok","timestamp":1654525064609,"user_tz":-120,"elapsed":6,"user":{"displayName":"Joep Franssen","userId":"14265939170258113406"}},"outputId":"527f6690-873f-4bb7-ded9-736c4373fc44"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'DOI_x', 'Retracted',\n","       'ID', 'Unnamed: 0.1.1.1', 'Unnamed: 0.1.1.1.1', 'Publication Type',\n","       'Authors',\n","       ...\n","       'Title + Abstract', 'Title + Abstract PP L', 'Main content PP L',\n","       'Discussion / Conclusion PP L', 'References PP L',\n","       'Title + Abstract PP S', 'Main content PP S',\n","       'Discussion / Conclusion PP S', 'References PP S', 'Journal_Name'],\n","      dtype='object', length=115)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"HuQWsM24FX-f"},"source":["## Train/test/val split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gTMHzcwELZV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654530879434,"user_tz":-120,"elapsed":289661,"user":{"displayName":"Joep Franssen","userId":"14265939170258113406"}},"outputId":"e09d4a54-7d9c-4029-b9fa-0ffd64a99a48"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","########### Title + Abstract PP S: \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_79']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","16/16 [==============================] - 19s 691ms/step - loss: 0.5525 - val_loss: 0.3421\n","Epoch 2/5\n","16/16 [==============================] - 9s 587ms/step - loss: 0.3024 - val_loss: 0.2563\n","Epoch 3/5\n","16/16 [==============================] - 9s 575ms/step - loss: 0.1772 - val_loss: 0.6577\n","Epoch 4/5\n","16/16 [==============================] - 9s 576ms/step - loss: 0.1136 - val_loss: 0.2947\n","Epoch 5/5\n","16/16 [==============================] - 9s 588ms/step - loss: 0.0355 - val_loss: 0.2220\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.94      0.90        33\n","           1       0.93      0.83      0.88        30\n","\n","    accuracy                           0.89        63\n","   macro avg       0.89      0.89      0.89        63\n","weighted avg       0.89      0.89      0.89        63\n","\n","[[31  2]\n"," [ 5 25]]\n","\n","####### NOW RUNNING ON THE EXTERNAL DATA SET #########\n","\n","[[ 88  44]\n"," [ 32 100]]\n","              precision    recall  f1-score   support\n","\n","           0       0.73      0.67      0.70       132\n","           1       0.69      0.76      0.72       132\n","\n","    accuracy                           0.71       264\n","   macro avg       0.71      0.71      0.71       264\n","weighted avg       0.71      0.71      0.71       264\n","\n","\n","########### Main content PP S: \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_99']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","16/16 [==============================] - 18s 688ms/step - loss: 0.6186 - val_loss: 0.5384\n","Epoch 2/5\n","16/16 [==============================] - 9s 588ms/step - loss: 0.4069 - val_loss: 0.3331\n","Epoch 3/5\n","16/16 [==============================] - 9s 575ms/step - loss: 0.2095 - val_loss: 0.4178\n","Epoch 4/5\n","16/16 [==============================] - 9s 590ms/step - loss: 0.1200 - val_loss: 0.2355\n","Epoch 5/5\n","16/16 [==============================] - 9s 588ms/step - loss: 0.0272 - val_loss: 0.2323\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.88      0.94        33\n","           1       0.88      1.00      0.94        30\n","\n","    accuracy                           0.94        63\n","   macro avg       0.94      0.94      0.94        63\n","weighted avg       0.94      0.94      0.94        63\n","\n","[[29  4]\n"," [ 0 30]]\n","\n","####### NOW RUNNING ON THE EXTERNAL DATA SET #########\n","\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2e9a7f0680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2e9a7f0680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["[[ 89  43]\n"," [ 22 110]]\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.67      0.73       132\n","           1       0.72      0.83      0.77       132\n","\n","    accuracy                           0.75       264\n","   macro avg       0.76      0.75      0.75       264\n","weighted avg       0.76      0.75      0.75       264\n","\n","\n","########### Discussion / Conclusion PP S: \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['dropout_119', 'pre_classifier', 'classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","16/16 [==============================] - 18s 690ms/step - loss: 0.6012 - val_loss: 0.5408\n","Epoch 2/5\n","16/16 [==============================] - 9s 587ms/step - loss: 0.3915 - val_loss: 0.3738\n","Epoch 3/5\n","16/16 [==============================] - 9s 588ms/step - loss: 0.1947 - val_loss: 0.3544\n","Epoch 4/5\n","16/16 [==============================] - 9s 575ms/step - loss: 0.1412 - val_loss: 0.3775\n","Epoch 5/5\n","16/16 [==============================] - 9s 588ms/step - loss: 0.0748 - val_loss: 0.3224\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.97      0.96        33\n","           1       0.97      0.93      0.95        30\n","\n","    accuracy                           0.95        63\n","   macro avg       0.95      0.95      0.95        63\n","weighted avg       0.95      0.95      0.95        63\n","\n","[[32  1]\n"," [ 2 28]]\n","\n","####### NOW RUNNING ON THE EXTERNAL DATA SET #########\n","\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f321ac7f200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f321ac7f200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["[[ 83  49]\n"," [ 27 105]]\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.63      0.69       132\n","           1       0.68      0.80      0.73       132\n","\n","    accuracy                           0.71       264\n","   macro avg       0.72      0.71      0.71       264\n","weighted avg       0.72      0.71      0.71       264\n","\n","\n","########### References PP S: \n","\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['dropout_139', 'pre_classifier', 'classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","16/16 [==============================] - 18s 687ms/step - loss: 0.6695 - val_loss: 0.6371\n","Epoch 2/5\n","16/16 [==============================] - 9s 587ms/step - loss: 0.5866 - val_loss: 0.5361\n","Epoch 3/5\n","16/16 [==============================] - 9s 588ms/step - loss: 0.4402 - val_loss: 0.4453\n","Epoch 4/5\n","16/16 [==============================] - 9s 576ms/step - loss: 0.3461 - val_loss: 0.4607\n","Epoch 5/5\n","16/16 [==============================] - 9s 576ms/step - loss: 0.2142 - val_loss: 0.6447\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.94      0.86        33\n","           1       0.92      0.73      0.81        30\n","\n","    accuracy                           0.84        63\n","   macro avg       0.86      0.84      0.84        63\n","weighted avg       0.85      0.84      0.84        63\n","\n","[[31  2]\n"," [ 8 22]]\n","\n","####### NOW RUNNING ON THE EXTERNAL DATA SET #########\n","\n","[[97 35]\n"," [91 41]]\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.73      0.61       132\n","           1       0.54      0.31      0.39       132\n","\n","    accuracy                           0.52       264\n","   macro avg       0.53      0.52      0.50       264\n","weighted avg       0.53      0.52      0.50       264\n","\n"]}],"source":["paper_sections = ['Title + Abstract PP S', 'Main content PP S', 'Discussion / Conclusion PP S', 'References PP S']\n","for section in paper_sections:\n","  print('\\n########### ' + section + \": \\n\")\n","\n","  if four_journal_train_data_set[section].isnull().values.any():\n","      nan_values = four_journal_train_data_set[four_journal_train_data_set[section].isnull()]\n","      four_journal_train_data_set = four_journal_train_data_set[~four_journal_train_data_set.ID.isin(nan_values.ID)]\n","\n","  raw_X = list(four_journal_train_data_set[section].values) # the texts --> X\n","  X = []\n","  y = list(four_journal_train_data_set.Retracted.values) # the labels we want to predict --> Y\n","\n","  for i in raw_X:\n","    if len(i.split(\" \")) > 420:\n","      head = i.split(\" \")[0:210]\n","      tail = i.split(\" \")[-210:]\n","      headandtail = \" \".join(head) + \" \".join(tail) \n","    else:\n","      headandtail = i\n","    X.append(headandtail)\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=1)\n","  X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n","\n","  tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n","  train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=510) # convert input strings to BERT encodings\n","  test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=510)\n","  val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=510)\n","\n","  train_dataset = tf.data.Dataset.from_tensor_slices((\n","      dict(train_encodings),\n","      y_train\n","  )).shuffle(buffer_size = 1000, seed=1).batch(16) # convert the encodings to Tensorflow objects\n","  val_dataset = tf.data.Dataset.from_tensor_slices((\n","      dict(val_encodings),\n","      y_val\n","  )).batch(64)\n","  test_dataset = tf.data.Dataset.from_tensor_slices((\n","      dict(test_encodings),\n","      y_test\n","  )).batch(64)\n","\n","  model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', \n","                                                            num_labels=len(labels))\n","  callbacks = [\n","          tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, \n","                        mode='min', baseline=None, \n","                        restore_best_weights=True)]\n","\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","  model.compile(optimizer=optimizer, loss=loss)\n","\n","  history = model.fit(train_dataset, \n","              epochs=5,\n","            callbacks=callbacks, \n","            validation_data=val_dataset,\n","            batch_size=16)\n","\n","  logits = model.predict(test_dataset)\n","  y_preds = np.argmax(logits[0], axis=1)\n","  print(classification_report(y_test, y_preds))\n","\n","  print(confusion_matrix(y_test, y_preds))\n","\n","  amountofpapers = len(two_journal_test_data_set)\n","  two_journal_test_data_set = two_journal_test_data_set.sample(amountofpapers)\n","\n","  print(\"\\n####### NOW RUNNING ON THE EXTERNAL DATA SET #########\\n\")\n","\n","  if two_journal_test_data_set[section].isnull().values.any():\n","    nan_values = two_journal_test_data_set[two_journal_test_data_set[section].isnull()]\n","    two_journal_test_data_set = two_journal_test_data_set[~two_journal_test_data_set.ID.isin(nan_values.ID)]\n","\n","  raw_X = list(two_journal_test_data_set[section].values[0:amountofpapers]) # the texts --> X\n","  X = []\n","  y = list(two_journal_test_data_set.Retracted.values[0:amountofpapers]) # the labels we want to predict --> Y\n","\n","  for i in raw_X:\n","    if len(i.split(\" \")) > 420:\n","      head = i.split(\" \")[0:210]\n","      tail = i.split(\" \")[-210:]\n","      headandtail = \" \".join(head) + \" \".join(tail) \n","    else:\n","      headandtail = i\n","    X.append(headandtail)\n","\n","  examples_encodings = tokenizer(X, truncation=True, padding=True)\n","  examples_encodings = tf.data.Dataset.from_tensor_slices((\n","                      dict(examples_encodings)\n","                        )).batch(64)\n","  pred_logits = model.predict(examples_encodings)\n","\n","  predictions = []\n","  for i, logits in enumerate(pred_logits[0]):\n","      prediction = np.argmax(logits)\n","      predictions.append(prediction)\n","\n","  print(confusion_matrix(y, predictions))\n","\n","  print(classification_report(y, predictions))\n","  \n","  save_directory = '/content/drive/MyDrive/Applied Data Science/Thesis/Code/Classifiers/BERT/saved_models/' +  section\n","  model.save_pretrained(save_directory)"]},{"cell_type":"code","source":["section = 'Main content PP S'\n","save_directory = '/content/drive/MyDrive/Applied Data Science/Thesis/Code/Classifiers/BERT/saved_models/' +  section\n","loaded_model = TFDistilBertForSequenceClassification.from_pretrained(save_directory)\n","\n","print(\"\\n####### NOW RUNNING ON THE EXTERNAL DATA SET #########\\n\")\n","\n","if two_journal_test_data_set[section].isnull().values.any():\n","  nan_values = two_journal_test_data_set[two_journal_test_data_set[section].isnull()]\n","  two_journal_test_data_set = two_journal_test_data_set[~two_journal_test_data_set.ID.isin(nan_values.ID)]\n","\n","raw_X = list(two_journal_test_data_set[section].values[0:amountofpapers]) # the texts --> X\n","X = []\n","y = list(two_journal_test_data_set.Retracted.values[0:amountofpapers]) # the labels we want to predict --> Y\n","labels = ['non retracted', 'retracted']\n","\n","for i in raw_X:\n","  if len(i.split(\" \")) > 420:\n","    head = i.split(\" \")[0:210]\n","    tail = i.split(\" \")[-210:]\n","    headandtail = \" \".join(head) + \" \".join(tail) \n","  else:\n","    headandtail = i\n","  X.append(headandtail)\n","\n","examples_encodings = tokenizer(X, truncation=True, padding=True)\n","examples_encodings = tf.data.Dataset.from_tensor_slices((\n","                    dict(examples_encodings)\n","                      )).batch(64)\n","pred_logits = loaded_model.predict(examples_encodings)\n","\n","predictions = []\n","for i, logits in enumerate(pred_logits[0]):\n","    prediction = np.argmax(logits)\n","    predictions.append(prediction)\n","\n","print(confusion_matrix(y, predictions))\n","\n","print(classification_report(y, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3eaGlTRpqkJ8","executionInfo":{"status":"ok","timestamp":1654530940769,"user_tz":-120,"elapsed":7803,"user":{"displayName":"Joep Franssen","userId":"14265939170258113406"}},"outputId":"3a3d496d-b3cc-4051-e59f-6f0722dc8319"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at /content/drive/MyDrive/Applied Data Science/Thesis/Code/Classifiers/BERT/saved_models/Main content PP S were not used when initializing TFDistilBertForSequenceClassification: ['dropout_99']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Applied Data Science/Thesis/Code/Classifiers/BERT/saved_models/Main content PP S and are newly initialized: ['dropout_179']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","####### NOW RUNNING ON THE EXTERNAL DATA SET #########\n","\n","[[ 89  43]\n"," [ 22 110]]\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.67      0.73       132\n","           1       0.72      0.83      0.77       132\n","\n","    accuracy                           0.75       264\n","   macro avg       0.76      0.75      0.75       264\n","weighted avg       0.76      0.75      0.75       264\n","\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Bert Head+Tail.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}